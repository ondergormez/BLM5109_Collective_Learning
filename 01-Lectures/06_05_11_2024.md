# 6. Week - 5 November 2024 Tuesday

Bugün regresyon ile ilgili konuları bitirip, stockhastice gradient descent ve quasi newton algoritmalarını işleyeceğiz.
Regresyondan (reg_mfa.pptx) devam ediyoruz.

# Regression

Normalde biz bir veri üzerinde çalışırken verinin aslında hangi order (first order, second order, ...) olduğunu bilmiyoruz.
* Bu yüzden bir model oluşturup (model tahmini), modelin doğruluğunu test etmemiz gerekiyor.
* Veriye dayalı olarak bir model oluşturuyoruz.

Hatayı tahminleyecek bir model kurabiliyorsanız, oluşturduğunuz modelde veriyi gerçekten temsil etmeyen modellenmemiş bir şeyler kaldığını söyleyebiliriz.
* Bizim modellememiz veriden daha az karmaşık ise, modelimizde underfitting olmuş olur.
* Modelimiz veriden daha karmaşık ise, modelimizde overfitting olmuş olur.
  * Yani modelimiz random hatayı veya veri üzerindeki gürültüyü de temsil etmiş olur.
  * Modelimiz eğitim kümesindeki dataya fazla uyumlu hale gelmiş olur.
  * Bu durumda modelimiz test kümesinde kötü performans gösterir.

Image Source: https://miro.medium.com/v2/resize:fit:4800/format:webp/1*_7OPgojau8hkiPUiHoGK_w.png

![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*_7OPgojau8hkiPUiHoGK_w.png)


Hatanın her bir değişkene (parametreye) göre türevinin alınmış halinin bulunduğu matrise jacobian matrisi denir.


# Stockhastice Gradient Descent (SGD)

# Quasi Newton Methods
sgd_quasi.pptx ile devam ediyoruz.
Quasi: Yaklaşık, taklit eden demek
Quasi Newton: Newton'un yaklaşık hali

Derste işlemediklerimiz;
* BFGS
* 

## AdaGrad
Bir Quasi Newton algoritmasıdır.

## Adam
AdaGrad ın gelişmiş versiyonudur.
* Adam va AdaGrad ın yaptığı şey SGD'in yaptığı yalpa ve salınımları (yani varyansın çok büyük olması) indirgemesidir.

# Ödev Açıklama

* 2 veriseti ve dolayısıyla, 2 mimari seçeceğiz
  * Veri kümesi çok küçük olursa algoritmalar arasındaki farkı göremezsiniz. Bunun için biraz daha büyük bir veriset üzerinde çalışılması gerekiyor.
* Veri kümesinde %90 başarı olmasa da olur. Burada önemli olan algoritmalar arasında oluşan farkları görmek istiyoruz. 
  * Gelmek istediğimiz nokta aynı veri seti üzerinde Stokastik Adam mı, SGD mi yoksa SGD with momentum mu başarılı.
* Stokastik süreci mini batch lerle oluşturmamızı istiyor. Bütün örnekleri değil 10 tanesi için işi yap, sonra 2. onluk olanı al.

Her veri seti için t-sne e 5 nokta * 3 algoritma (adam, sgd, sgd with momentum) = 15 gidiş yolunu vereceğiz ki sonuçlar aynı uzayda görünsün.