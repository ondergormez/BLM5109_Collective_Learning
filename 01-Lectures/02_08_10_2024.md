# 2. Week - 8 October 2024 Tuesday

[kolektif öğrenme nedir?](https://drive.google.com/file/d/1t0vZsea8vQ6DKDtjVQAsbnH6SbFFe5kk/view) slaytından işliyoruz. 8. slayttan başladık.

## Bias - Varyans

Standart Sapma: Bir şeyin kendi ortalamasından ne kadar saptığı
Bias: Ortalamanızla hedef nokta arasındaki fark.

Bias: Aynı dağılımdan gelen farklı eğitim kümleri için üretilen modellerin kararlarının ortalamasının gerçeğe uzaklığı

Varyans: Aynı dağılımdan gelen farklı eğitim kümeleri için üretilen modellerin kararlarının kendi ortalamalarına uzaklığı


Bias=0 ise tahminleri birleştirmek hatayı azaltırmı?
* Bias 0 ise hepsinin ortalaması hedef nokta üzerindedir demek
* Fakat her bir tekil nokta hedef noktadan uzaklaşmış olabilir
* Bu nedenle dataları birleştirmek ortalamayı hedef noktaya taşır.
* Hata azalmış olur.

Varyans=0 ise
* Tahminler hep üst üste demektir.
* Bunları birleştirmek tahminlerin ortalamasını değiştirmez
* Bu nedenle hatayı azaltmaz

## What is Optimization?

[optimizasyon-1](https://drive.google.com/file/d/1JYfcxNJLBXbePrU9_i75QN6U3gd4mY_P/view) slaytına başladık.

* Minimizing the result.

Model ailesini biz söylüyoruz.
* Model ailesine karar veren bir mekanizma karşımızda olmayacak.
* Daha sonra biz katsayılarını değiştirerek modelin performansını arttırmaya çalışıyoruz.
* Her modelde bir varsayım vardır. "Bu data bu modelle modellenir." gibi.

# Optimization Techniques

## Non Derivative Techniques
Non derivative tekniklere girmeyeceğiz.

* Hill climbing
* Genetic algorithms
* Simulated annealing

## Derivative Techniques

* Gradient Descent
* Newton Raphson
* LM
* BFGS

## Ana Kavramlar Linear, Nonlinear, Convex Functions

Convex: Seçtiğim iki nokta arasında oluşturduğum doğru her zaman denklemin eğrisinin üzerinde ise convex fonksiyondur deriz.

![](https://www.probabilitycourse.com/images/chapter6/Convex_b.png)
Source: https://www.probabilitycourse.com/images/chapter6/Convex_b.png


![](https://fmin.xyz/docs/theory/convex_function.svg)
Source: https://fmin.xyz/docs/theory/convex_function.svg

## How to Find Minimum Point?

* Use derivative
* Go to the negative direction of derivative
* But, step size?

## Gradient Descent
* 2. dereceden denklemlerde çok iyi çalışır.
* Convex olmayan 3. derece denklemler için yerel minimum da takılıp kalabilir.
* Step size ın çok büyük olması (Mesela 1 olması) converge etmesini (minimum noktaya yaklaşmasını) önleyebilir.
* Türevi kullanmak bize gideceğimiz yön tayininde faydalı oluyor.
* Convex fonksiyonlar için başlangıç noktasının önemi yok.
  * Nerden başlıyorsan başla global minimum a converge ediyor
* Convex olmayan (3. derece ve üzeri) fonksiyonlarda başlangıç noktası önemli
  * Başladığımmız yer local minimumda kalmamıza (türev 0 olduğu için) burayı aşamamamıza neden olabilir. 

Slaytımızı bitirdik.