# 9. Week - 26 November 2024 Tuesday

# 1) Bagging Yöntemi


Karar ağaçlarını büyütürsek (karmaşıklığı yüksek, düğüm sayısı fazla ve derin) düşük başarı ve yüksek çeşitlilik, daha genel bırakırsak yüksek başarı ve az çeşitlilik olur.

 karmaşıklı



Farklı bir konu; Multi Layered Perceptron: Linear 2 ye ayırıyor

* Bagging bir birinin sonuçlarını beklemediği için çok kolay paralelize edilebilir.
* 

# 2) Ardışık Topluluklarla Öğrenme (Boosting)

Copilot says;
* Boosting, zayıf öğrenicileri bir araya getirerek güçlü bir öğrenici oluşturmayı amaçlar.
* Zayıf öğrenicileri bir araya getirirken, bir önceki öğrenicinin hatalarını düzeltmeye çalışır.
* Boosting algoritmaları, zayıf öğrenicileri bir araya getirirken, bir önceki öğrenicinin hatalarını düzeltmeye çalışır.


My notes:
* Boosting de paralel bir şekilde çalışmaz.
* Birinci eğitilir sonra ikincisi eğitilir.

## Adaboost

* Bir şeyin seçilme olasılığı yüksek ve hata yapıldıysa bunun error rate i daha yüksek ağırlıklı bir hatası oluyor.
* 

# 3) Rastsal Altuzaylar - Random Subspace

* 100 özellikten random 10 tanesini seçip onlar üzerinden bir model oluşturuyoruz.
* Bu şekilde 10 farklı model oluşturuyoruz.
* Bu modellerin tahminlerini birleştiriyoruz.
* d: özellik sayısı
  *

* gain ratio: bunun sonucu a
* distribution mappping:
* centroid:
* unsupervised clustering:
* supervised clustering:
* central axis projection:
* perceptron:
* support vector machine:

10 base learner var. 5 tanesi a 5 tanesi b dedi. Ne yapacağız? rastgele birini seçmek zorundayız.
* Sırf bu nedenle bile 11 tane base learner oluşturmak daha mantıklı olabilir.

Ensemble lara ne kadar ekleme yaparsanız yapın overfitting olmaz. Çünkü birbirlerinin hatalarını düzeltmeye çalışıyorlar.

# Kalan Ödev 1 Sunumlarının Yapılması

Dersin kalan kısmında geçen hafta yapılamayan 4 tane ödev sunumu yapıldı.